
## 증권 거래소

> 증권 거래소는 지연 시간, 처리량, 안정성에 대해 요구사항이 엄격하다.

- 가용성 : 최소 99.99% 작동
- 결함내성 : 빠른 복구 메커니즘 필요
- 지연 시간 : p99가 중요
- 보안 : 계정 관리 및 KYC확인 Ddos 체크 등

#### KYC (Know your client)

말 그대로 "고객을 알아야 한다"는 원칙 -> 주로 금융기관, 암호화폐 거래소, 핀테크 서비스 등에서 사용하는 본인 확인 절차

- 신분증 (주민등록증, 여권 등)
- 실명 계좌 인증
- 휴대폰 본인 인증
- 얼굴 사진과 신분증 대조 (셀카 인증)

시스템에 접근하는 사용자가 실제로 본인이 맞는지 검증하는 신원 확인(Identity Verification) 과정
인증(Authentication)과는 조금 다른 게, KYC는 "처음 가입할 때 이 사람이 진짜 누구인지" 확인하는 거고, 인증은 "이미 등록된 사용자가 맞는지" 확인


### FIX (Financial Information Exchange) Protocol

> 주식 거래를 위해 만들어졌고, 지금은 전 세계 증권사, 거래소, 헤지펀드, 자산운용사 등에서 광범위하게 사용되는 업계 표준 규칙
 
- TCP/IP 위에서 동작하는 세션 기반 프로토콜으로 구성
- 태그=값 형태의 간단한 구조 (예: 35=D|49=SENDER|56=TARGET|...)
- 시퀀스 번호로 메시지 유실/중복 감지

```
8=FIX.4.4|9=176|35=D|49=BUY_SIDE|56=SELL_SIDE|34=215|
52=20231215-09:30:00|11=ORDER123|21=1|55=AAPL|54=1|
60=20231215-09:30:00|38=100|40=2|44=150.50|10=128|
```

#### FIX와 Ticker

티커(`Ticker`)란?

에디슨이 개량한 "stock ticker" 기계에서 유래. 이 기계가 종이 테이프에 주가 정보를 찍어내는데, 전신(telegraph)으로 데이터를 전송하다 보니 글자 수를 최소화해야 했음.

![](https://velog.velcdn.com/images/cksgodl/post/6c0effe0-840e-4452-8cdc-8ae5bd0a8878/image.png)

그래서 회사 이름을 1~3글자로 축약한 게 티커 심볼의 시작
이러한 짧은 관행을 그대로 가져와 FIX프로토콜로 발전이 됨

```
55=AAPL   ← Symbol (티커)
48=037833100  ← SecurityID (CUSIP 등 고유 식별자)
22=1      ← SecurityIDSource (1=CUSIP, 4=ISIN 등)
```

## 개략적 설계안

![](https://velog.velcdn.com/images/cksgodl/post/3aceb51f-1758-42b1-9ecc-3a086de487da/image.png)

- `클라이언트 게이트웨이`는 입력 유효성 검사, 속도 제한, 인증, 정규화 등 기본적인 게이트키핑 기능 수행
   - 코로케이션을 통해 거래소 데이터 센터에 서버를 둘 수 있다.
- `위험 관리자` : 위험 관리자가 설정한 규칙에 따라 주문에 따른 위험성 점검 수행
- `시퀀서` : 주문 및 집행 기록을 일정 순서로 정렬 (메시지 큐의 역할)
  - 체결 엔진을 결정론적으로 만든다. 
  - 각각의 주문에 순서 ID를 붙여 정렬하여 체결 엔진에게 전달한다.
- `체결 엔진` : 매수측과 매도 측에 각각 하나씩 두개의 집행 기록 생성
  - 결정론적이어야 한다. 주문 순서가 같으면 체결 엔진이 만드는 집행 기록 순서는 언제나 동일해야 한다.

### 코로케이션 (Colocation)

> 코로케이션(Colocation, Colo)은 거래소 데이터센터 안에 트레이딩 서버를 직접 설치하는 것

- 레이턴시(지연시간) 최소화
- 빛의 속도도 거리에 비례해서 지연
- KRX(한국거래소)도 부산 데이터센터에서 코로케이션 서비스를 제공

### 시퀀서의 작동 방식

> 시퀀서(Sequencer)는 거래소에서 모든 주문에 공정한 순서를 부여하는 핵심 컴포넌트

수천 개의 트레이딩 서버에서 동시에 주문이 들어오는데, 이에 대한 순서 처리

```
[HFT A] ──┐
[HFT B] ──┼──▶ [시퀀서] ──▶ [매칭엔진] ──▶ 체결
[HFT C] ──┘
```

기본 작동 방식

- 단일 진입점(Single Point of Entry)
모든 주문이 시퀀서를 통과해야 해. 시퀀서가 도착 순서대로 시퀀스 번호를 찍어줌.
  ```
  Order A → Seq #1 (09:30:00.000001)
  Order B → Seq #2 (09:30:00.000002)
  Order C → Seq #3 (09:30:00.000003)
   ```

- 타임스탬핑
나노초(ns) 단위의 고정밀 타임스탬프를 부여해. GPS 또는 원자시계와 동기화된 시간을 사용하는 경우가 많아.

- 공정성 모델들
![](https://velog.velcdn.com/images/cksgodl/post/8f1597b4-4bfe-408d-b910-46791139a4b8/image.png)
  - IEX의 특이한 접근: Speed Bump
IEX는 의도적으로 350μs 지연을 넣어서 HFT의 속도 이점을 무력화
  - 물리적으로 광케이블을 감아서 지연을 만듦. 이러면 마이크로초 단위 경쟁이 의미가 없어짐

## 상세 설계안

### 성능을 위한 고정 쓰레드

주문관리자에서는 특정 폴링 스레드를 단일 스레드 및 특정 CPU코어에 고정시켜 해당 작업만 수행하도록 한다.

이를 통해 콘텍스트 스위칭이 일어나지 않고, 락을 활용할 필요가 없어 경합도 존재하지 않는다.

__어플리케이션 루프 스레드가 CPU를 너무 잡아먹지 않도록 각 작업에 걸리는 식나을 적절히 조정해야 한다.__

### 성능을 위한 단일 서버와 mmap
  
> 모든 거래소가 지연 시간을 더 낮추는 경쟁에 나서면서 오랜 시간 동안 검증된 설계안이 있는데 모든 것을 동일한 서버에 배치하여 네트워크를 통하는 구간을 없애는 것이다 
> ![](https://velog.velcdn.com/images/cksgodl/post/00c2779e-7f0f-4106-8c08-2d444a157944/image.png)
>
> 이에 따라 같은 서버내 컴포넌트 간 통신은 이벤트 저장소는 `mmap`을 통한다.

__`mmap`은 파일을 프로세스의 가상 주소 공간에 매핑하는 시스템콜을 의미한다.__

`/dev/shm`의 정체는 `tmpfs`라는 **RAM 기반 파일시스템**이다. 파일처럼 보이지만 실제로는 커널이 관리하는 메모리 영역이다.

`tmpfs`는 `tmporary file system`의 약자이다.

"임시 파일시스템"을 의미하며, `RAM`을 사용해서 디스크 없이 동작하는 파일시스템이다. 재부팅하면 내용이 전부 날아간다.

```
bash$ df -T /dev/shm
Filesystem     Type   Size  Used Avail Use% Mounted on
tmpfs          tmpfs  16G   100M  16G   1%  /dev/shm
```

#### 일반 파일 mmap vs /dev/shm mmap 비교:

```
[ 일반 파일 mmap: /home/user/data.bin ]

가상주소 ──▶ 페이지 테이블 ──▶ 페이지 캐시 ──▶ 디스크
                                    ↑
                              page fault 시
                              디스크에서 읽어옴
```

```
[ /dev/shm mmap: /dev/shm/shared_data ]
가상주소 ──▶ 페이지 테이블 ──▶ RAM (tmpfs)
                                  ↑
                            처음부터 메모리에 있음
                            디스크 개입 없음
```

#### 실제 활용을 위해

- **`Chronicle Map` :  `mmap`을 활용하는 라이브러리**
  - https://github.com/OpenHFT/Chronicle-Map
  
```
kotlinval map = ChronicleMap
    .of(String::class.java, Long::class.java)
    .name("my-map")
    .entries(1_000_000)
    .createPersistedTo(File("/dev/shm/my-map.dat"))

// 다른 프로세스에서도 같은 파일로 접근 가능
```

- `Chronicle Queue` (저지연 IPC 메시징) 
   - https://github.com/OpenHFT/Chronicle-Queue

```
kotlinval queue = ChronicleQueue.single("/dev/shm/my-queue")
val appender = queue.acquireAppender()
appender.writeText("message")
```

- Java NIO 직접 사용

```
kotlinval path = Paths.get("/dev/shm/shared-buffer")
val channel = FileChannel.open(path, READ, WRITE, CREATE)
val buffer: MappedByteBuffer = channel.map(
    FileChannel.MapMode.READ_WRITE, 0, 4096
)

buffer.putInt(0, 12345)  // 다른 프로세스와 공유됨
```

----

책에서는 `Chronicle Queue`와 같이 `mmap`을 이벤트 버스로 활용하여 성능으 높이고 있다. 

![](https://velog.velcdn.com/images/cksgodl/post/48e931f3-1b53-441c-833c-27959f56f8d6/image.png)

### 고가용성

`99.99%`의 고가용성은 장애시간이 하루에 8.64초를 넘기면 안된다.

- 결정론적 특성을 활용하여 복구
- 주 체결 엔진, 부 체결 엔진을 분리한다.

#### `mmap`기반 이벤트 버싱의 경우에는 래프트 알고리즘 활용

`mmap` 자체를 `RPC`를 통해 클러스터로 만든다.
이를 지원하는 라이브러리로는 `Aeron`이 존재한다.

- https://github.com/aeron-io/aeron

```
┌─────────────────────────────────────────────────────────┐
│                                                         │
│  Publisher ──▶ [mmap Log Buffer] ──▶ Media Driver ──┐  │
│                    (로컬 저장)           │           │  │
│                                          ▼           │  │
│                                    UDP/IPC 전송      │  │
│                                          │           │  │
│  Subscriber ◀── [mmap Log Buffer] ◀─────┘           │  │
└─────────────────────────────────────────────────────────┘
```
- mmap 기반 로컬 버퍼 (zero-copy)
- UDP unicast/multicast 또는 IPC 전송
- 마이크로초 단위 지연
- LMAX 거래소에서 실제 사용

```
┌──────────────┐      복제       ┌──────────────┐
│ Chronicle    │ ────────────▶  │ Chronicle    │
│ Queue        │   (TCP/네트워크) │ Queue        │
│ (mmap 기반)   │                │ (replica)    │
│ Server A     │                │ Server B     │
└──────────────┘                └──────────────┘
```
