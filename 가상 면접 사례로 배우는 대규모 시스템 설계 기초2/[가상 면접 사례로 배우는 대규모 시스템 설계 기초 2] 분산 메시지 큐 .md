# 분산 메시지 큐

메시지 큐를 사용할 때의 이점

- 결합도 완화 : 작고 독립적인 블록/컴포넌트들 간의 강한 결합을 끊고, 독립적으로 운영 가능
- 규모 확장성 개선 : 트래픽 부하에 따라 producer, consumer 조절
- 가용성 개선 : 시스템의 특정 컴포넌트에 장애가 발생해도 다른 컴포넌트는 큐와 계속 상호작용을 이어갈 수 있음
- 성능 개선 : 비동기 통신 가능

## 분산 메시지큐의 간단한 흐름

![](https://velog.velcdn.com/images/cksgodl/post/a3efdf37-d74c-4440-8e42-cf114fda14b8/image.png)

- 생산자는 메시지를 메시지 큐에 발행한다.
- 소비자는 큐를 구독하고 구독한 메시지를 소비한다.
- 메시지 큐는 생산자와 소비자 사이의 결합을 느슨하게 한다.


- 발행-구독 모델을 활용
   - 토픽은 메시지를 주제별로 정리하는 데 사용된다.
메시지를 보내고 받을 때는 토픽에 보내고 받게 된다.
토픽에 전달된 메시지는 해당 토픽을 구독하고 있는 모든 소비자에게 전달된다.


> Q. 토픽에 보관되는 데이터 양이 많아지면?
>
> A. 토픽은 여러 파티션으로 분할한 다음에 메시지를 모든 **파티션(샤딩)**에 균등하게 나눠 보낸다. 따라서 파티션 개수를 늘려 해결할 수 있다. (파티션은 메시지 큐 클러스터 내의 서버에 고르게 분산 배치한다.)

![](https://velog.velcdn.com/images/cksgodl/post/f1e88e4b-a457-4333-90b1-fc7e49954486/image.png)



>- 각 토픽의 파티션은 FIFO 큐처럼 동작 ⇒ **같은 파티션 내에서는 순서가 유지된다**
>- 파티션은 가장 작은 저장 단위 ⇒ 미리 충분히 할당해두어야 동적으로 늘리는 일을 피할 수 있다
>    - 처리 용량을 늘리고 싶으면, 소비자를 더 추가하면 된다
>- Produce : 생산자가 보낸 메시지는 해당 토픽의 파티션 중 랜덤으로 보내지게 되고, 파티션키를 활용시 이를 고정할 수 있다. 

카프카의 경우 특정 파티션의 메시지는 한 그룹 안에서 오직 한 소비자만 읽을 수 있다. 이러한 제약사항은 파티션내에서의 메시지 순서만 유지한다. 

**즉, 메시지가 여러 파티션에 분산되어 전송되면, 전체 순서 보장은 불가능
**

특정 데이터의 순서 보장이 필요하면 파티션 키를 활용해 하나의 파티션을 활용해야 한다.

### 개략적 설계안

![](https://velog.velcdn.com/images/cksgodl/post/c30ff78e-81af-49c9-8047-b0796f847a69/image.png)

> 위 그림의 핵심 서비스 및 저장소

- 브로커 : 파티션들을 유지하는 서버
- 저장소
   - 데이터 저장소 : 메시지는 파티션 내 데이터 저장소에 보관 
      - **로컬 디스크 파일 시스템 활용**
   - 상태 저장소 : 소비자 상태 보관
      -  KRaft
        오프셋의 경우는 **Kafka 내부의 특별한 토픽 (`__consumer_offsets`)**
   - 메타데이터 저장소 : 토픽 설정, 토픽 속성 등 보관
      - **ZooKeeper**
- 조정 서비스 (coordination service) -> 주로 주키퍼
   - 서비스 탐색 : 어떤 브로커가 살아있는가
   - 리더 선출 : 브로커 중 하나는 컨트롤러 역할을 담당, 한 클러스터에 반드시 활성 상태인 컨트롤러가 하나 있어야 함 → 이는 파티션 배치를 책임지게 됨

> **KRaft란?**
>
> KRaft = Kafka Raft Metadata Mode
Kafka에서 ZooKeeper를 제거하고,
자체적으로 메타데이터를 관리하기 위한 구조


---


분산 메시지큐를 설계하기 위해 **데이터의 장기 보관 요구사항**을 만족하면서** 높은 대역폭을 제공**하기 위한 3가지 특징이 있다.

- 회전 디스크(rotational disk)의 높은 순차 탐색 성능과 현대적 운영체제가 제공하는 적극적 디스크 캐시 전략(aggresive disk caching strategy)을 잘 이용하는 디스크 기반 자료 구조(on-disk data structure) 활용
- 메시지가 생상자로부터 소비자에게 전달되는 순간까지 아무 수정 없이도 전송이 가능하도록 하는 메시지 자료 구조 설계 및 활용
전송 데이터 양이 막대한 경우, 메시지 복사에 드는 비용을 최소화 하기 위함
- **일괄 처리(batching)**를 우선하는 시스템 설계
   - 생산자 - 메시지 일괄 전송
   - 소비자 - 메시지 일괄 수신


> **일괄 처리(batching)**
일괄 처리가 **성능 개선**에 중요한 이유
OS가 여러 메시지를 한 번의 네트워크 요청으로 전송할 수 있도록 함 → 값비싼 네트워크 왕복 비용 제거
브로커가 여러 메시지를 한 번에 로그에 기록하면 더 큰 규모의 순차 쓰기 연산이 발생하고, OS가 관리하는 디스크 캐시에서 더 큰 규모의 연속된 공간을 점유하게 됨 → 더 높은 디스크 접근 대역폭에 달성

> 카프카의 속도가 빠른 이유 -> 쓰기 우선 로그(Write-Ahead Log, WAL)의 활용
>
> ![](https://velog.velcdn.com/images/cksgodl/post/2c194b23-df26-41aa-ab56-5da4d0b9d41d/image.png)

- WAL은 새로운 항목이 추가되기만 하는(append-only) 일반 파일
- WAL에 대한 접근 패턴은 읽기/쓰기 전부 순차적이다.
- **접근 패턴이 순차적일 때 디스크는 아주 좋은 성능을 보이고**, 회전식 디스크 기반 저장장치는 큰 용량을 저렴한 가격에 제공한다.
- 세그먼트의 크기가 일정 한계에 도달하면 새 활성 세그먼트 파일이 만들어져 새로운 메세지를 수용하고, 종전까지 활성 상태였던 세그먼트 파일은 다른 나머지 세그먼트 파일과 마찬가지로 비활성 상태로 바뀐다.
   - **세그먼트 컴팩션**
- 비활성 세그먼트는 읽기 요청만 처리한다.
- 낡은 비활성 세그먼트 파일은 보관 기한이 만료되거나 용량 한계에 도달하면 삭제해 버릴 수 있다.
  - **용량, 기간**등 설정


| 성능 요소                   | 설명                                                  |
| ----------------------- | --------------------------------------------------- |
| ✅ **Zero Copy**         | 데이터를 디스크 → 네트워크로 복사할 때 커널 메모리만 활용함. → CPU 부하 ↓ 속도 ↑ |
| ✅ **압축 지원**             | Snappy, LZ4, GZIP 등 → 전송/저장 속도 및 용량 절약              |
| ✅ **OS Page Cache 사용**  | Kafka는 JVM 내 캐시 안 쓰고 OS 캐시를 최대한 활용함 → 메모리 효율적       |
| ✅ **Broker는 Stateless** | Kafka는 상태관리를 최소화하고 로그만 담당 → 병렬 처리 가능                |


### 메시지 자료 구조

- 메시지 구조
  - 높은 대역폭 달성의 열쇠, 생산자-메시지 큐-소비자 사이의 계약(contract)

*메시지가 중간에 변경된다면, 값비싼 copy 연산이 필요하고, 이는 시스템 전반의 성능에 치명적인 영향을 미칠 수 있다*

### 생산자 측 작업 흐름

> 라우팅 계층을 도입하여 ‘적절한’ 브로커에 메시지를 보낸다.

![](https://velog.velcdn.com/images/cksgodl/post/dddec869-5992-4fd2-bf2b-1a8f52a42b27/image.png)

1. 생산자는 우선 메시지를 라우팅 계층으로 보낸다
2. 라우팅 계층은 메타데이터 저장소에서 사본 분산 계획을 읽어 자기 캐시에 보관한다
3. 메시지가 도착하면 라우팅 계층에서 파티션-1의 리더 사본으로 보낸다
4. 리더 사본이 메시지를 받고 해당 리더를 따르는 다른 사본은 해당 리더로부터 데이터를 받는다
5. 충분한 수의 사본이 동기화되면 리더는 데이터를 디스크에 기록(commit)한다
   → 데이터가 소비 가능 상태가 되는 시점
6. 기록이 끝나면 생산자에게 회신을 보낸다

#### 단점

라우팅 계층을 도입함으로써 네트워크 노드가 하나 더 늘어나게 되므로 오버헤드가 발생 → 네트워크 전송 지연이 늘어난다
일괄 처리가 가능하면 효율을 많이 높일 수 있는데, 이는 고려하지 않은 설계


![](https://velog.velcdn.com/images/cksgodl/post/f3cb0756-ffa8-4a46-8ac7-e1d37a4b9360/image.png)

#### 생산자 측 버퍼 및 라우팅 (단점 보완)

- 라우팅 계층을 생산자 내부로 편입
   - 버퍼 도입
   - 생산자 클라이언트 라이브러리의 일부로 생산자에 설치

#### 장점

- 네트워크 X → 전송 지연이 줄어든다
- 생산자는 메시지를 어느 파티션에 보낼지 결정하는 자신만의 로직을 가질 수 있음
- 전송할 메시지를 버퍼 메모리에 보관했다가 목적지로 일괄 전송 가능 → 대역폭이 높아진다 (but, 그만큼 기다려야 하므로 응답 속도는 느려짐)

### 소비자 측 작업 흐름

소비자는 특정 파티션의 오프셋을 주고 해당 위치에서부터 이벤트를 묶어 가져온다

![](https://velog.velcdn.com/images/cksgodl/post/bc641233-ba68-401f-8750-40d0802711da/image.png)

- Consumer A는 0~9까지
- Consumer B는 0~11까지

#### 풀 모델의 활용 : 소비자 → 브로커에서 데이터 직접 호출

- 장점
  - 메시지 소비 속도는 소비자가 결정 (실시간 or 일괄)
  - 소비 속도가 느려지더라도, 소비자가 이를 늘리거나 생산 속도를 따라 잡을 때까지 기다리는 방식으로 해결 가능
  - 일괄 처리에 적합 - 소비자가 마지막으로 가져간 로그 위치 다음의 (모든 or 정해진 개수 만큼) 모든 메시지를 한번에 가져갈 수 있음

- 단점
  - 브로커에 메시지가 없어도 소비자가 계속 데이터를 끌어가려 시도할 것 
     - 컴퓨팅 자원의 낭비
       이는 롱 폴링 지원을 통해 일정 시간 기다리도록 함으로써 조금은 극복할 수 있다.

대부분의 메시지큐는 아래와 같은 형식으로 진행된다.

![](https://velog.velcdn.com/images/cksgodl/post/c9e45a5b-899c-4218-815a-d3120282983b/image.png)

1. 그룹-1에 합류하고 토픽-A를 구독하길 원하는 소비자가 해당 그룹 이름을 해싱하여 접속할 브로커 노드를 찾는다
2. 코디네이터는 해당 소비자를 그룹에 참여시키고 파티션-2를 해당 소비자에 할당한다
   파티션 배치 정책 : 라운드-로빈, 범위 기반 정책 등
3. 소비자는 마지막으로 소비한 오프셋 이후 메시지를 가져온다
4. 소비자는 메시지를 처리하고 새로운 오프셋을 브로커에 보낸다


### 소비자 리밸런싱

> 코디네이터가 매우 중요한 역할을 하며, 소비자에게 발생한 장애를 박동 신호가 사라지는 현상을 통해 감지한다. 또한, 장애를 감지하면 재조정 프로세스를 시작하여 파티션을 재배치한다.

아래와 같은 상황일 때 발생한다.

- 새로운 소비자 합류
- 기존 소비자가 그룹 탈퇴
- 어떤 소비자에 장애 발생
- 파티션 조정

>#### 스프링의 KafkaConsumer

![](https://velog.velcdn.com/images/cksgodl/post/4539abb0-5cc5-4eb8-948f-df98e03cc909/image.png)

각각 컴포넌트의 자세한 내용은 [KafkaConsumer Client Internals - D2](https://d2.naver.com/helloworld/0974525)

- 데이터 프로세싱과 Health Check 별도 쓰레드에서 진행한다. 
- poll.interval.ms 및 heartbeat.interval.ms를 별도로 설정한다.
   - poll.inervals.ms란? -> 해당 시간 내에 poll이 호출되지 않으면 Group에서 제외
   - session.timeout.ms란? -> 해당 시간동안 HeartBeat이 도착하지 않으면 Group Coordinator 는 해당 Consumer를 Group 에서 제외한다 (보통 하트비트 3초, 세션 10초로 유지한다.)
- max.poll.records 설정을 크게하여 poll.interval.ms를 넘기도록 poll을 못하면 해당 컨슈머는 그룹에서 제외되어 리밸런싱이 발생한다.
   - 기본값은 500이다.

![](https://velog.velcdn.com/images/cksgodl/post/56e6c7e3-c1b5-4b2f-8b4e-09e6dc44173c/image.png)



> Kafka가 Rebalancing 되는 과정 중에서는 모든 Consuming( Data Fetching ) 작업이 멈춰지는 STW(Stop The World) 현상이 이루어지게 된다

### 상태 저장소

메시지 큐 브로커의 상태 저장소에는 무엇이 저장될까?
- 소비자에 대한 파티션의 배치 관계
- 각 소비자 그룹이 각 파티션에서 마지막으로 가져간 메시지의 오프셋
   - 소비자는 오프셋으로 설정된 그 다음 위치부터 가져가므로, 앞의 메시지는 이미 읽어갔음을 의미한다
- **Kafka 브로커(KRaft)**를 활용한다.

### 메타데이터 저장소

토픽 설정, 속성 정보를 보관하는 곳

- 파티션 수, 메시지 보관 기관, 사본 배치 정보
- 특징 - 자주 변경되지 않으며, 양이 적다 + 높은 일관성을 요구

이는 주키퍼가 적합하여 주키퍼를 활용한다.

> 주키퍼(ZooKeeper)
>
>계층적 키-값 저장소 기능을 제공하는, 분산 시스템에 필수적인 서비스
분산 메시지 큐를 설계하는 데 아주 유용
분산 설정 서비스, 동기화 서비스, 이름 registry 등으로 이용됨

![](https://velog.velcdn.com/images/cksgodl/post/01b21a59-1ae3-42c9-b774-0a7f978dce2b/image.png)

주키퍼가 도입 이후 달라진 점?

- 메타데이터와 상태 저장소는 주키퍼를 이용해 구현한다
- 브로커는 이제 메시지 데이터 저장소만 유지하면 된다
- 주키퍼가 브로커 클러스터의 리더 선출 과정을 돕는다

### 복제

![](https://velog.velcdn.com/images/cksgodl/post/346ac257-9062-4ae0-8e3b-570825834cf0/image.png)

분산 시스템에서 하드웨어 장애는 흔한 일이므로, 데이터의 영구적 보관과 높은 가용성을 보장하기 위해 복제(replication)를 활용할 수 있다.

> 생산자는 파티션에 메시지를 보낼 때 리더에게만 보내며, 다른 사본은 리더에서 새 메시지를 지속적으로 가져와 동기화한다

### 사본 동기화

복제한 사본들 모두를 어떻게 동기화할까?

![](https://velog.velcdn.com/images/cksgodl/post/1f8c5d24-f831-4504-977a-d976195a1ed5/image.png)

**동기화된 사본(In-Sync Replicas, ISR) : 리더와 동기화된 사본을 일컫는 용어**

- 리더는 항상 ISR 상태다
- 리더 상태를 충분히 따라잡으면 ISR이 될 수 있다
   - replica.lag.max.messages ≥ (단순 사본에 보관된 메시지 개수 - 리더) : ISR
- 각 파티션 담당 리더는 자기 사본들이 어느 메시지까지 가져갔는지 추적하여 ISR 목록을 관리한다
- 합의 오프셋 : 해당 오프셋 이전에 기록된 모든 메시지가 이미 ISR 집합 내 모든 사본에 동기화가 끝났음을 의미
   → ISR이 되면 새로운 메시지를 가져올 수 있다.
   → ISR 요건을 만족하는 사본에서는 메시지를 가져가지 않는다.

#### ACK=all

생산자는 모든 ISR이 메시지를 수신한 뒤에 ACK 응답을 받는다

→ 메시지를 보내기 위한 시간이 길어지지만, 매시지의 영속성 측면에서는 best

#### ACK=1

생산자는 리더가 메시지를 저장하고 나면 바로 ACK 응답을 받는다

→ 응답 지연이 개선되지만, ACK를 보낸 직후 리더에 장애가 생기면 해당 메시지는 사본에 반영되지 못하므로 소실됨

#### ACK=0

생산자는 보낸 메시지에 대한 수신 확인 메시지를 기다리지 않고 계속 메시지를 전송하며 어떤 재시도도 하지 않는다

→ 낮은 응답 지연을 달성하기 위해 메시지 손실을 감수.
→ 지표 수집, 데이터 로깅 등 처리해야 하는 메시지 양이 많고 데이터 손실에 상관 없는 경우에 적합

### 규모 확장성

각 컴포넌트의 규모를 확장하기 위해서는 어떻게 해야하는가?

#### 생산자

새로운 생산자를 추가/삭제함으로써 규모 확장성 달성

#### 소비자

같은 소비자 그룹 내의 소비자 추가/삭제, 장애는 재조정 메커니즘이 처리
 소비자 그룹, 재조정 메커니즘의 소비자 측의 규모 확장성과 결함 내성을 보장

#### 브로커

브로커 노드가 추가/삭제될 때 사본을 재배치
**브로커 컨트롤러가 한시적으로 시스템에 설정된 사본 수보다 많은 사본을 허용하도록 하는 것
**
#### 파티션

- 파티션 추가 : 지속적으로 보관된 메시지는 새로운 파티션으로 이동하지 않지만, 그 이후 오는 메시지는 모든 파티션에서 보관되어야 함
- 파티션 삭제 : 퇴역시킨다는 결정이 난 파티션 외의 파티션으로 새로운 메시지를 보관하도록 하고, 해당 파티션을 아직 사용중인 소비자가 있을 수 있기 떄문에 바로 제거하지 않고 일정 시간 유지함
   - 실제로 제거되는 시점에는 생산자 그룹에서 재조정 작업 필요

### 메시지 전달 방식

#### 최대 한 번(at-most once)

메시지가 전달 과정에서 소실되더라도 다시 전달되는 일은 없다

- 생산자 : ACK=0
- 소비자 : 메시지를 읽고 처리하기 전에 오프셋부터 갱신

#### 최소 한 번(at-least once)

같은 메시지가 한 번 이상 전달될 수는 있으나 메시지 소실은 발생하지 않는다

- 생산자 : 메시지를 동기/비동기적으로 보낼 수 있음. ACK=1, ACK=all
- 소비자 : 데이터를 성공적으로 처리한 뒤에만 오프셋 갱신 (미처 오프셋을 갱신하지 못하고 죽었다가 다시 시작하면 중복 처리될 여지가 있음)
→ 메시지는 브로커나 소비자에 한 번 이상 전달될 수 있다

#### 정확히 한 번(exactly once)

중복을 허용하지 않으며, 중요한 데이터를 다루는 애플리케이션에 특히 중요하다

*가장 구현이 까다로움*


### 기타 고려사항

#### 카프카 프로토콜 Kafka Protocol의 활용

> Kafka 클라이언트(Producer, Consumer, Admin 등)와 Kafka Broker가
서로 통신하기 위해서는 네이티브 바이너리 프로토콜을 활용

 Kafka 클라이언트와 브로커가 TCP 기반으로 고성능 통신하기 위해 만든 바이너리 프로토콜이며, 
자체 API Key 체계와 버전 관리


#### 메시지 소비 재시도 

제대로 받아 처리하지 못한 메시지는 다시 처리를 시도해야 함

> **Dead Letter Queue (DLQ) 패턴**
> 
> 재시도를 여러 번 실패한 메시지를 최종적으로 DLQ 토픽에 보내어 처리 및 모니터링 한다.


> **이력 데이터 아카이브 **
>
> 시간 기반 or 용량 기반 로그 보관 리텐션이 있을 때, 이미 삭제된 메시지를 다시 처리하길 원하는 소비자가 존재하는 경우에 요구되는 기능
>
> Kafka Connect Sink Connector 등으로 데이터를 HDFS, S3 등에 백업진행


## 정리

![](https://velog.velcdn.com/images/cksgodl/post/b7c35009-3520-4c58-af12-6dfe09db02ff/image.png)
