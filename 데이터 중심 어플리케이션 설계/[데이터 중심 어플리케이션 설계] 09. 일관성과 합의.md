네트워크 기반의 분산시스템은 엉망이다. 이러한 시스템을 견뎌낼 알고리즘이 필요하다.

가장 중요한 추상화 중 하나는 **합의**이다. 모든 노드가 어떤 것에 동의하게 만드는 것이다. 추후에 알아보겠지만 신뢰성있게 합의에 도달하는 것은 매우 까다롭다.  

이러한 시스템 수십년간 연구되어 왔고, 자료도 많다. 이에 대해 알아보자.

## 일관성 보장

데이터베이스간의 일관성 보장은 중요하다. 데이터 베이스는 대부분 **최소한 최종적 일관성**을 제공한다. 

> 최소한 최종적 일관성 : 쓰기를 멈추고 불특정 시간 기다리면 결국 모든 읽기는 같은 값을 반환

바꿔 말하면 불일치는 일시적이며 결국 스스로 해소한다.  하지만,  이것은 매우 약한 보장이며 **언제** 수렴할지 아무도 모른다. 

데이터 시스템은 선택적으로 더 강한 일관성 모델을 제공할 수 있다. (이는 내결함성이 낮는 등 트레이드 오프가 있다.) 따라서 몇 가지 일관성 모델을 알아보고 잘 맞는 것을 활용하자.

## 선형성

모든 클라이언트가 하나의 레플리카만 본다는 추상화를 진행하면 복제 지연을 걱정할필요 없다.

이것은 **선형성**을 뒷받침하는 아이디어다. 이는 **원자적 일관성(atomic consistency)**, **강한 일관성(strong consistency)**, **외부 일관성(external consistency)** 라고도 한다. 선형성의 정의는 미묘하지만 기본 아이디어 자체는 시스템에 데이터 복사본 하나만 있고, 그 데이터를 대상으로 수행하는 모든 연산은 원자적인 것 처럼 보이게 하는 것이다.

다시 말해 선형성 보장은 **최신성 보장(recency guarantee)**이다.

![](https://velog.velcdn.com/images/cksgodl/post/a9b0c1c3-b256-4a55-9cb5-c1bc1c12b380/image.png)

- 위 시스템은 비선형이라 혼란스럽다.

### 시스템에 선형성을 부여하는 것은 무엇인가?

![](https://velog.velcdn.com/images/cksgodl/post/b28d6994-ab52-4519-8be6-107028eed676/image.png)

위 그림에서 클라이언트B는 0을 받을수도, 1을 받을수도 있다. 읽기와 쓰기가 동시에 발생했기 떄문이다. 하지만 이는 선형성을 설명하기엔 부족하다. 쓰기가 진행되는 동안 값이 오래된 값과 새로운 값 사이에서 왔다갔다 할 수 있기 떄문이다. (복제 데이터 시스템이기 때문에)

따라서 선형성을 보장하기 위해서는 아래와 같이 제약조건을 추가해야 한다.

![](https://velog.velcdn.com/images/cksgodl/post/038f0560-7643-4c95-a087-e32a803ca836/image.png)

클라이언트A가 1을 반환하면 B또한 역시 1을 반환해야 한다. 이 타이밍에 대한 개별 연산을 시각화하면 다음과 같다.

![](https://velog.velcdn.com/images/cksgodl/post/53d0a692-d5b5-4c52-8c8c-f8c7d01aed07/image.png)

- `cas(x, 0, 3)`은 `compare-and-set` 연산이다.

위의 그림에서는 연산이 실행됐다고 생각하는 시점이 수직선으로 표시되어 있다. 각 실행은 선현성을 보장해야 한다. (마지막 읽기는 선형적이지 않다.)

_위의 그림은 트랜잭션을 제공하지는 않는다. 언제든지 값을 바꿀 수 있다._

> **선형성 vs 직렬성 **
>
> * 직렬성 : 트랜잭션의 격리 속성이다. 각 오퍼레이션이 순서대로 실행되는 것처럼 동작하도록 보장해준다.
> * 선형성 : 선형성은 연산을 트랜잭션을 묶지 않고, 최신 값을 반환한다. 이는 쓰기 스큐와 같은 현상이 일어난다.

### 잠금과 리더 선출

단일 리더 복제를 사용하면 **스플릿 브레인** 문제를 해결해야 한다. 한 가지 방법은 잠금을 사용하는 것이다. 모든 노드가 시작할 때 잠금 획득을 시도하고, 성공한 노드가 리더가 된다. (잠금은 선형적이어야 한다.) 또한 모든 노드는 어떤 노드가 잠금을 소유하는지 동의해야 한다.

분산 잠금과 리더 선출을 구현하기 위해
* Apache ZooKeeper
* etcd

와 같은 코디네이션 서비스가 종종 사용된다. 

### 제약 조건과 유일성 보장

이러한 잠금은 유일해야 한다. 

### 채널 간 타이밍 의존성

이미지 업로드 API를 예로 들어보자. 사용자들은 이미지를 올릴 수 있고, 백그라운드에서는 사진 다운로드를 빠르게 할 수 있도록 저해상도(썸네일)로 바꾸는 로직이 다음 그림과 같이 동작한다.

![](https://velog.velcdn.com/images/cksgodl/post/e5575aeb-6fc3-447a-b8c4-e2a7d6a862d2/image.png)

이 파일 저장 서비스가 선형적이라면 이 시스템은 잘 동작한다. 하지만, 선형적이지 않다면 경쟁 조건의 위험이 있다. 

 메세지 큐(3, 4단계)가 저장 서비스 내부의 복제(2 단계)보다 빠를 경우, 5~6단계를 수행하려고 할 때, 그 이미지의 과거 버전 혹은 아무것도 없을 수도 있다. 이 경우, 만약 과거 버전의 이미지를 처리한다면 이미지 불일치 문제가 발생할 것이다. 

이 문제는 웹 서버와 크기 변경 모듈 사이에 두 가지 다른 통신 채널, 파일 저장소와 메시지 큐가 있기에 발생한다. 선형성의 최신성 보장이 없을 경우 이 두 채널 사이에 경쟁 조건이 발생할 수 있다. 

선형성은 경쟁 조건을 회피하는 유일한 방법은 아니다. 

### 선형성 시스템 구현하기

가장 간단한 방법은 단일 노드를 활용하는 것인데 이는 내결함성을 제공할 수 없다. 따라서 복제 방법을 살펴보며 선형적으로 만들 수 있을지 비교해보자.

- 단일 리더 복제(선형적이 될 수도 있음)
  - 리더 및 팔로워에서 실행한 읽기는 선형적일 수도 있다.
  - 스플릿 브레인을 통한 리더 또한 선형성을 위반하기 쉽다.
- 합의 알고리즘 (선형적)
  - 이는 스플릿 브레인과 복제본이 뒤쳐지는 문제를 막으며, 선형성을 안전하게 구현할 수 있다.
- 다중 리더 복제(비선형적)
  - 여러 노드가 동시에 쓰기를 처리하고 쓰여진 노드가 복제되기에 선형적이지 않다.
- 리더 없는 복제(아마도 비선형적)
  - 정족수 읽기와 쓰기를 통해 일관성을 제공한다고 생각할 수 있으나, 동기화된 시계에 의존하기에 이또한 비선형적일 수 있다.

### 선형성과 정족수

![](https://velog.velcdn.com/images/cksgodl/post/9da3c00c-bde3-4ad7-8720-700a83f7aa34/image.png)
- 엄격한 정족수를 사용하지만 비선형적인 실행
- w : 3, r = 2, n = 3

> 정족수 조건이 만족(w + r > n)됨에도 이 실행은 선형적이지 않다. 즉 리더없는 복제에서는 선형성을 제공하지 않는다고 보는게 안전하다.

### 선형성의 비용

![](https://velog.velcdn.com/images/cksgodl/post/e58d9344-1ccf-495a-a21e-8e9e6fde698b/image.png)
- 네트워크가 끊기면 선형성과 가용성 사이에서 선택해야만 한다.

네트워크 연결이 끊겨도 각 데이터센터에 리더가 있다면 네트워크 연결이 복구되면 데이터가 복구된다. (단일리더 설정이면 이는 불가능하다.) 

### CAP 정리

CAP 정리는 선형성과 가용성에 대한 트레이드 오프를 의미하며 다음과 같다. 

- 애플리케이션에서 **선형성**을 요구하고 네트워크 문제 때문에 일부 복제 서버가 다른 복제 서버와 연결이 끊기면 일부 복제 서버는 연결이 끊긴 동안은 요청을 처리할 수 없다. 네트워크 문제가 고쳐질 때까지 기다리거나 오류를 반환해야하는데 어떤 방법도 **가용성**이 없다. 
- 애플리케이션에서 **선형성**을 요구하지 않는다면 각 복제 서버가 다른 복제서버(Ex: 다중 리더)와 연결이 끊기더라도 독립적으로 요청을 처리하는 방식으로 쓰기를 처리할 수 있다. 
  이 경우 애플리케이션은 네트워크 문제에 직면해도 가용한 상태를 유지하지만, 선형적이지 않다.

### 선형성과 네트워크 지연

> 현실에서 실제로 선형적인 시스템은 드물다. 
>
> 최신 다중코어 CPU의 램(RAM)조차 선형적이지 않는다. 
이유는 모든 CPU 코어가 저마다 메모리 캐시와 저장 버퍼를 갖기 때문이다.  메모리 접근은 기본적으로 캐시로 먼저 가고 변경은 메인 메모리에 비동기로 기록된다. 캐시에 있는 데이터를 접근하는 게 메인 메모리로 가는 것보다 훨씬 더 빠르기에 이런 특성은 최신 CPU에서 좋은 성능을 내는 데 필수적이다. 

선형성을 제공하는 데이터베이스는 드물다. 선형성 보장을 제공하지 않기로한 이유는 주로 성능이다. 선형성은 느리다.

## 순서화 보장

![](https://velog.velcdn.com/images/cksgodl/post/d5c71759-9a24-47bd-b638-67228afe7a4a/image.png)

선형성 레지스터는 데이터 복사본이 하나만 있는 것처럼 동작하고 모든 연산이 어느 시점에 원자적으로 효과가 나타나는 것처럼 보인다고 했다. 

이 정의는 연산들이 순서대로 실행된다는 것을 암시한다. 

순서화와 선형성, 합의 사이에는 깊은 연결 관계가 있다. 

### 순서화와 인과성

> 인과성(因果性)은 원인과 결과의 규칙적인 관계를 말합니다. 인과성을 바탕으로 한 인과관계는 두 변수 간의 관계를 나타내며, 한 변수가 다른 변수에 직접적인 영향을 미치는 것을 의미합니다. 

순서화는 인과성을 보존하는 데 도움을 준다.

- 일관된 순서로 읽기에서 대화의 관찰자가 질문에 대한 응답을 먼저 보고 나서 응답된 질문을 보게했는데, 질문이 답변됐다면 반드시 그 질문이 먼저 있었어야 한다. 이를 가리켜 질문과 답변 사이에 **인과적 의존성(causal dependency)**이 있다고 말한다.
- 네트워크 지연 문제로 어떤 쓰기가 다른 쓰기를 추월할 수 있다.

인과성은 이벤트에 순서를 부과한다. 결과가 나타나기 전에 원인이 발생한다, 메시지를 받기 전에 메시지를 보낸다, 답변하기 전에 질문을 한다. 등

시스템이 인과성에 의해 부과된 순서를 지키면 그 시스템은 **인과적으로 일관적(casually consistent)**라고 한다. 데이터베이스를 읽어서 어떤 조각을 봤다면 그보다 인과적으로 먼저 발생한 어떤 데이터도 볼 수 있어야 한다.

### 인과적 순서가 전체 순서는 아니다.

> **전체 순서(total order)** 또는 선형 순서 는 두 요소가 비교 가능한 부분 순서 입니다. 즉, 전체 순서는 이진 관계 입니다.

전체 순서와 부분 순서의 차이점은 다른 데이터베이스 일관성 모델에 반영된다.

#### 선형성

선형성 시스템에서는 연산의 전체 순서를 정할 수 있다. 모든 시스템이 데이터 복사본 하나만 잇는 것 처럼 동작하고 모든 연산이 원자적이면 두 연산에 대해 항상 둘 중 하나가 먼저 실행됐다고 말할 수 있다.

#### 인과성

두 연산 중 어떤 것도 다른 것보다 먼저 실행되지 않았다면 두 연산이 동시적이라고 말한다. 달리 말하면 두 이벤트에 원인과 결과의 관계가 있으면 이들은 순서가 있지만 이들이 동시에 실행되면 비교할 수 없다. 인과성이 전체 순서가 아닌 부분 순서를 정의한다는 뜻이다. 

어떤 연산들은 서로에 대해 순서를 정할 수 있지만 어떤 연산은 비교할 수 없다.

---

이 정의에 따르면 선형성 데이터베이스에는 동시적 연산이 없다. 타임라인이 있고, 타임라인에 따라 전체 순서가 정해져야 한다. 데이터베이스는 동시성 없이 단일 데이터 복사본에 연산을 실행된것 처럼 보장한다.

깃은 인과적 의존성 그래프와 매우 유사하다. 하나의 커밋은 다른 것보다 일직선 상에서 나중에 실행되고, 때때로 브랜치를 만들도 만들어진 커밋을 합칠 때 머지 커밋이 생성된다.

### 선형성은 인과적 일관성보다 강하다.

> **선형성은 인과성**을 내포한다.

시스템을 선형적으로 만드는 것은 매력적이지만, 비용과 성능적으로 해가 된다. 많은 분산 데이터 시스템들은 선형성을 포기해서 더 좋은 성능을 달성하지만 사용하기는 더 어렵다.

하지만 절충도 가능하다. 선형성은 인과성을 보장하는 유일한 방법은 아니다. 선형성이 필요한 것 처럼 보이는 시스템도 사실 필요한 것은 인과적 일관성이며 이는 더 효율적으로 구현될 수 있다.

### 인과적 의존성 담기

> **인과적 의존성은 결과가 원인에 의존하는 것을 의미합니다.**
> 
> * 반사실적 조건문: C가 발생하지 않았더라면 E가 발생하지 않았다와 같은 조건문을 말합니다. 
* 인과율: 인과관계를 설명하는 수학적 표현으로 자연법칙이라고도 합니다. 
* 결정론: 모든 사건에는 원인이 있고, 우주 만물은 절대적으로 인과법에 의존한다는 주장입니다. 

인과성을 유지하기 위해 어떤 연산이 어떤 다른 연산보다 먼저 실행됐는지를 알아야 한다. 이를위해 **버전 벡터**를 활용한다. 쓰기나 읽기를 실행할 때 어쩐 버전에서 읽었는지 추적한다.

### 일련번호 순서화

인과성을 실제로 추적하는 것은 실용성이 떨어진다. 따라서 **일련번호**나 **타임스탬프**를 활용하여 이벤트의 순서를 정한다. 단, 이전장에서 보았듯 시계는 믿을 수 없기에 논리적 시계인 증가하는 카운터를 활용할 수 있다.

> 증가하는 카운터를 활용해 **인과성에 일관적인** 일련번호를 생성할 수 있다.

단일 리더의 경우는 OpLog에 일련번호를 등록하고 팔로워가 이를 따라가며 일관성을 제공할 수 있다.

### 비인과적 일련번호 생성기

단일 리더가 없다면 연산에 사용할 일련번호를 생성하는 방식이 다르다.

- 각 노드가 자신만의 독립적인 일련번호 집합을 생성한다. (노드가 2개이면 한 노드는 짝수, 한 노드는 홀수만 생성한다.)
- 각 연산에 물리적 시계에서 얻은 타임스탬프를 붙일 수 있다. 
- 일련번호 블록을 미리 할당할 수 있다. (노드 1 : 0~1000, 노드 2 : 1001~2000)

> 결론: 비인과적 일련번호는 우리가 기대하는 순서를 보장하지 않는다

- "A가 먼저 발생했으면, A의 번호가 B보다 작아야 한다" → ✅ (보장 안 됨❌)
- "작은 번호일수록 먼저 발생한 연산이다" → ✅ (보장 안 됨❌)

즉, 일련번호가 우리가 기대하는 순서대로 증가하지 않기 때문에 "인과성이 깨졌다"라고 말할 수 있다.

### 램포트 타임스탬프

인과성에 일관적인 일련번호를 생성하는 간단한 방법으로 램포트 타임스탬프(Lamport timestamp)가 있다.

![](https://velog.velcdn.com/images/cksgodl/post/b64a77e0-e594-4119-9df1-64723c72834b/image.png)

1. 각 노드는 고유 식별자를 갖고 각 노드는 처리한 연산 개수를 카운터로 유지한다. 
   - 램포트 타임스탬프는 (카운터, 노드ID)의 쌍이다. 
2. 두 노드는 때때로 카운터 값이 같을 수 있지만 타임스탬프에 노드ID를 포함시켜 중복되지 않게 한다.
3. 램포트 타임스탬프는 물리적 일 기준 시계와 관련 없지만 전체 순서화를 제공한다.
4. 모든 노드와 모든 클라이언트가 카운터 값 중 최댓값을 추적하고 모든 요청에 그 최댓값을 포함시킨다.
5. 노드가 자신의 카운터 값보다 큰 최대 카운터를 가진 요청이나 응답을 받으면 바로 자신의 카운터를 그 최댓값으로 증가시킨다. 

이는 짝수/홀수 카운터와 본질적으로 같다. 모든 연산에 최대 카운터 값이 따라다니는 이 방법은 램포트 타임스탬프로부터 얻은 순서가 인과성에 일관적이도록 보장해준다.

램포트 타임스탬프는 전체 순서화로부터 두 연산이 동시적인지 혹은 인과적으로 의존성이 있는지를 알 수는 없다. 버전 벡터보다 램포트 타임스탬프가 좋은 점은 크기가 더 작다는 것이다.

### 타임스탬프 순서화로는 충분하지 않다.

램포트 타임스탬프로는 분산 시스템의 여러 공통 문제를 해결하기에는 부족함이 있다.

예를 들어, 사용자명으로 사용자 계정을 유일하게 식별할 수 있도록 보장해야 하는 시스템을 고려해보자. 두 사용자가 동시에 동일한 사용자명으로 계정을 생성하려고 하면 둘 중 한 명은 성공하고 다른 사람은 실패해야 한다.

타임스탬프가 더 낮은것을 성공한것으로 판단하는 식으로 시스템에서 사용자명 생성 연산을 모두 모아 타임스탬프를 비교해서 결정할 수 있다.  하지만, 노드가 사용자로부터 사용자명 생성 요청을 막 받고 그 요청의 성공 혹은 실패 여부를 당장 결정해야 할 때는 램포트 타임스탬프로는 부족하다. (다른 노드가 동시에 동일한 사용자명으로 계정 생성을 처리하고 있는지와 다른 노드가 그 연산에 어떤 타임스탬프를 배정할지 알지 못한다.)

__결국 문제는 연산의 전체 순서가 연산을 모두 모은 뒤에 드러난다는 점이다.__

다른 노드가 어떤 연산을 생성했지만 그것이 무엇인지 아직 알 수 없다면 연산의 최종 순서를 만들어낼 수 없다. 이 문제를 해결하기 위해서는 연산의 전체 순서 뿐 아니라 그 순서가 언제 확정되는지도 알아야 한다.

언제 전체 순서가 확정되는지 알아야 한다는 아이디어는 **전체 순서 브로드캐스트**에서 알아보자.

### 전체 순서 브로드캐스트

분산 시스템에서 모든 노드의 연산의 전체 순서가 동일하도록 합의하기는 까다롭다. (단일 리더를 제외하고) 단일리더일 때는 문제가 발생할 수 있다.

1. 처리량이 단일 리더 처리량을 넘어설 때 어떻게 시스템 확장을 할 것인가
2. 리더에 장애가 발생했을 때 어떻게 장애 복구를 처리할 것인가

분산 시스템 분야에서 이 문제는 **전체 순서 브로드캐스트(total order broadcats)**나 **원자적 브로드캐스트(atomic broadcast)**로 알려져 있다.

> **순서화 보장의 범위**
>
> 샤딩된 클러스터는 각 클러스터마다 순서를 유지한다. 이는 전체 클러스터에서 일관성을 보장하지 않는다는 뜻이다. (가능하긴 하지만 추가적인 코디네이션 필요)

#### 📡 전체순서 브로드캐스트 (Total Order Broadcast, TOB)

> 📌 전체순서 브로드캐스트란?
분산 시스템에서 메시지를 모든 프로세스가 같은 순서로 받도록 보장하는 방법
램포트 타임스탬프만으로는 전체 순서를 보장할 수 없기 때문에 추가적인 알고리즘 필요

- 신뢰성 있는 전달(reliable delivery)
  - 어떤 메시지도 손실되지 않는다. 메시지가 한 노드에 전달되면 모든 노드에도 전달된다.
- 전체 순서가 정해진 전달(totally ordered delivery)
  - 메시지는 모든 노드에 같은 순서로 전달된다.

### 전체 순서 브로드캐스트 사용하기

모든 메시지가 DB에 쓰기를 나타내고 모든 복제 서버가 같은 쓰기 연산을 같은 순서로 처리하면 복제 서버들은 서로 일관성 있는 상태를 유지한다.

이 원리를 **상태 기계 복제(state machine replication)**라고 한다.

#### 브로드 캐스트 + 램포트 타임스탬프

- 모든 프로세스가 자신의 타임스탬프와 함께 메시지를 보냄
- 메시지를 받은 모든 프로세스는 타임스탬프 순서대로 정렬 후 적용
- 장점: 중앙 노드 없이 분산 환경에서 동작 가능
- 단점: 동시성 해결을 위해 추가적인 메시지 지연 발생 가능

```
P1: 메시지(A, T1) 브로드캐스트
P2: 메시지(B, T2) 브로드캐스트
P3: 메시지(C, T3) 브로드캐스트
모든 노드에서 (T1 → T2 → T3) 순서대로 처리!
```

#### 🔹 램포트 타임스탬프와 전체순서 브로드캐스트의 관계

램포트 타임스탬프만으로는 전체순서 브로드캐스트를 보장할 수 없음 (전체 순서 브로드캐스트는 틈이 없는 순열을 생성한다.)
따라서 전체순서를 위해 멀티캐스트 + 정렬 기법이 필요 (일반적으로 **"램포트 타임스탬프 + 메시지 정렬"**을 통해 구현)

- 램포트 타임스탬프는 각 이벤트의 논리적 순서만 결정 (전체 순서 X)
- 전체순서 브로드캐스트는 모든 프로세스가 동일한 순서로 메시지를 받도록 강제

## 분산 트랜잭션과 합의

합의를 어디에 쓰일까

- 리더 선출
- 원자적 커밋
   - 모든 노드가 커밋되어야지 커밋을 성공시킨다.
   
### 단일 노드에서 분산 원자적 커밋으로

단일 노드의 원자적 커밋은 흔히 저장소 엔진에서 구현된다. 따라서 트랜잭션 커밋은 순서에 의존한다. 중요한 점은 트랜잭션의 커밋이나 어보트를 결정하는 시점은 레코드 쓰기를 마치는 시점이다. 따라서 데이터 베이스를 원자적으로 만들어 주는 것은 단일 장치(디스크에 부착된 디스크 드라이브 컨트롤러)이다.

트랜잭션에 여러 노드가 관련되면 복잡해진다. 각 노드에 커밋 요청을 보내고 각 노드에서 성공적으로 트랜잭션을 커밋하는 것으로는 충분하지 않다. (보통 다른 노드에서 실패하는 경우가 많음)

- 노드마다 다른 커밋상태 공존가능
- 네트워크 상태에 따라 커밋이 전달되지 않을 수 있음

따라서 모든 노드가 커밋될 것이라고 확신이 되고난 후에 커밋해야 한다.

### 2단계 커밋 소개

2단계 커밋은 분산 트랜잭션을 달성하는 알고리즘이다. 데이터베이스는 흔히 `2PC` 또는 `XA 트랜잭션`을 활용한다. 

![](https://velog.velcdn.com/images/cksgodl/post/e3a4e328-e5e4-413f-960e-4aed755fd19f/image.png)

> **2PC와 2PL에 관하여**
>
> 2PC는 데이터베이스에서 원자적 커밋을 제공하고, 2PL은 직렬성 격리를 제공한다. 이는 완전히 분리된 개념이다.
> - 원자적 커밋 : 분산 데이터베이스에서의 트랜잭션
> - 직렬성 격리 : 동시 실행이 마치 하나씩 순차적으로 실행된 것과 같은 결과를 보장하는 격리 수준
   
2PC 트랜잭션은 
애플리케이션이 여러 DB 노드에서 데이터를 읽고 쓰면서 시작된다. (이 때 DB노드를 트랜잭션의 참여자(participant)라고 부른다.) 
애플리케이션이 커밋할 준비가 되면 코디네이터가 1단계를 시작한다. 

1. 각 노드에 준비 요청을 보내 커밋할 수 있는지 물어본다.
2. 코디네이터는 참여자들의 응답을 추적한 뒤 
   a. 모두 준비가 됐다면 2단계에서 커밋요청을 보내고 커밋이 이뤄진다.
   b. 준비가 안 된 노드가 **하나라도** 있다면 2단계에서 모든 노드에 어보트 요청을 보낸다.


### 약속에 관한 시스템

자세한 동작 과정은 다음과 같다.

1.
애플리케이션은 분산 트랜잭션 시작을 원할 때 코디네이터에게 트랜잭션ID(이하 txid)를 요청한다. 
이  txid는 전역적으로 유일하다.
2.
애플리케이션은 각 참여자에게 단일 노드 트랜잭션을 시작하고 단일 노드 트랜잭션에 전역적으로 유일한 txid를 붙인다.  모든 읽기와 쓰기는 단일 노드 트랜잭션 중 하나에서 실행된다. 이 단계에서 문제가 생기면(ex: 노드가 죽거나 타임아웃) 코디네이터나 참여자 중 누군가가 어보트할 수 있다. 
3.
애플리케이션이 커밋할 준비가 되면 코디네이터는 모든 참여자에게 전역 txid로 태깅된 준비 요청을 보낸다. 이 과정에서도 문제가 생기면 코디네이터는 모든 참여자에게 해당 txid로 어보트 요청을 보낸다. 
4.
참여자가 준비 요청을 받으면 모든 상황에서 분명히 트랜잭션을 커밋할 수 있는지 확인한다. 여기에는 모든 트랜잭션 데이터를 디스크에 쓰는 것(죽거나 전원 장애나 디스크 공간이 부족한건 커밋을 거부하는데 용인되는 변명이 아니다)과 충돌이나 제약 조건 위반을 확인하는 게 포함된다. 코디네이터에게 네 라고 응답함으로써 노드는 요청이 있으면 트랜잭션을 문제 없이 커밋할 것이라 약속한다. 
5.
코디네이터가 모든 준비 요청에 대해 응답을 받았을 때 트랜잭션의 커밋/어보트 여부를 최종 결정한다. 코디네이터는 추후 죽는 경우에 어떻게 결정했는지 알 수 있도록 그 결정을 디스크에 있는 트랜잭션 로그에 기록해야하며 이를 커밋 포인트라 한다.
6.
코디네이터의 결정이 디스크에 쓰여지면 모든 참여자에게 커밋이나 어보트 요청이 전송된다. 이 요청에 문제가 생기면 성공할 때까지 영원히 재시도해야 한다. 커밋하기로 결정이 되었다면 재시도 횟수와 상관없이 무조건 커밋을 해야 한다. 한 참여자가 죽으면 그 참여자가 복구될 때 커밋된다.
   - **단일 장애 지점이 존재한다.**

이 프로토콜은 두 개의 중대한 돌아갈 수 없는 지점이 있다.

1.
커밋 포인트: 코디네이터가 모든 준비 요청에 대해 응답을 받고, 트랜잭션의 커밋/어보트 여부를 최종 결정하고 이를 디스크에 쓰는 지점. 이 지점 이후로는 결정을 바꿀 수 없다.
2.
커밋 요청: 코디네이터의 결정이 디스크에 쓰여지고, 모든 참여자에게 커밋이나 어보트 요청이 전송된 이후에는 커밋이나 어보트 요청을 바꿀 수 없다.
이러한 약속이 2PC의 원자성을 보장한다. 

#### 좀더 쉽게...

 **⚡ 2PC의 두 가지 단계**
2PC는 **"준비(Prepare)" → "커밋(Commit)"** 이렇게 두 단계로 진행된다.

* **✅ 1단계: 준비 단계 (Prepare Phase)**

1. **조정자(Coordinator)가 모든 참여 노드(Participants)에게 "트랜잭션을 커밋할 준비가 되었는지" 요청**  
   - `PREPARE` 메시지 전송  
2. **각 참여 노드는 준비 완료 여부를 응답**
   - 준비 가능하면 `"YES"` (OK)  
   - 준비 불가능하면 `"NO"` (ABORT)  
   - 모든 참여 노드가 `"YES"`를 보내야 다음 단계 진행 가능  

💡 이 단계에서 아직 데이터 변경이 확정되지 않음 🚨  

* **✅ 2단계: 커밋 단계 (Commit Phase)**
3. **모든 참여 노드가 `"YES"`를 응답했다면 조정자가 `"COMMIT"` 메시지를 전송**  
   - 각 노드는 트랜잭션을 **영구 저장(Commit)**  
4. **어떤 노드라도 `"NO"`(ABORT)를 응답했다면 조정자가 `"ROLLBACK"` 메시지를 전송**  
   - 각 노드는 **트랜잭션을 취소(Rollback)**  

✅ **결과적으로 모든 노드가 동일한 결과를 보장** (Commit or Rollback)  


### 코디네이터 장애

*2PC 도중 코디네이터가 죽을 경우 어떻게 될까? *

- 코디네이터가 준비 요청을 보내기 전에 장애가 나면 안전하게 로컬에서 트랜잭션 어보트가 가능하다.
- 코디네이터가 준비 요청을 보낸 후에 참여자가 요청을 받고 네라고 투표했다면 일방적으로 어보트할 수 없다.

전자라면 문제가 안되지만 후자인 경우 투표를 한 참여자는 코디네이터로부터 커밋/어보트 여부에 대한 회신을 받을 때까지 기다려야 한다. 그리고 이런 상태에 있는 참여자의 트랜잭션을 **의심스럽다(in doubt) or 불확실하다(uncertain)**고 한다.


![](https://velog.velcdn.com/images/cksgodl/post/4dd3f91b-a698-4797-bc08-1972d898adfb/image.png)

이런상황에서 2PC가 완료될 수 있는 유일한 방법은 코디네이터가 복구되길 기다리는 것 뿐이다. 이것이 코디네이터는 참여자들에게 커밋이나 어보트 요청을 보내기 전에 디스크에 있는 트랜잭션 로그에 자신의 커밋이나 어보트 결정을 쓰는 이유다. 복구될 때 해당 로그를 읽어서 복구한다.

### 3단계 커밋

_2단계 커밋은 2PC가 코디네이터가 복구하기를 기다리느라 멈출 수 있다는 사실 때문에 블로킹 원자적 커밋 프로토콜이라 불린다._

이론상으로는 노드에 장애가 나도 멈추지 않도록 원자적 커밋 프로토콜을 논블로킹하게 만들 수 있다. (현실에서 구현하긴 어렵다.) 따라서 2PC의 대안으로 3단계 커밋(3PC)이라는 알고리즘이 제안됐다. 

![](https://velog.velcdn.com/images/cksgodl/post/e330d9e0-675a-4c89-b68f-e54de6fa391e/image.png)

하지만 3PC는 지연에 제한이 있는 네트워크와 응답 시간에 제한이 있는 노드를 가정한다. 기약 없는 네트워크 지연과 프로세스 중단이 있는 대부분의 실용적 시스템에서 3PC는 원자성을 보장하지 못한다.

일반적으로 논블로킹 원자적 커밋은 완벽한 장애 감지기(perfect failure detector), 즉 노드의 활성화 여부를 구별할 수 있는 신뢰성 있는 메커니즘이 필요하다. 하지만 기약 없는 네트워크 지연이 있는 네트워크에서 타임아웃은 신뢰성 있는 장애 감지기가 아니다. 

이러한 이유로 2PC가 계속 사용되고 있다. 

### 현실의 분산 트랜잭션

이는 안전성을 보장하지만 운영상 문제를 일으키고, 성능이 저하된다. 많은 분산 시스템은 분산 트랜잭션을 제공하지 않는다.

아래의 두 종류의 분산 트랜잭션은 혼용되어 사용된다.

- 데이터베이스 내부 분산 트랜잭션
   * 트랜잭션에 참여하는 모든 노드는 동일한 DB를 사용한다.
   * Ex: VoltDB, MySQL, 클러스터의 NDB 저장소 엔진
* 이종 분산 트랜잭션
   * 이종(heterogeneous) 트랜잭션에서 참여자들은 둘 혹은 그 이상의 다른 기술이다. 두 가지 서로 다른 벤더의 DB일 수도 있고, 메세지 브로커처럼 비데이터베이스 시스템일 수도 있다.


### 정확히 한 번 메시지 처리

이종 분산 트랜잭션은 MSA 모듈들을 강력하게 통합시킨다. 분산 트랜잭션이 적용되면 서로 다른 장비에서 실행되는 두 가지 무관한 기술이더라도 결합될 수 있다.

> 트랜잭션이 실패하면 메시지 브로커는 메시지를 나중에 안전하게 전달할 수 있다. **결과적으로 정확히 한 번(exactly once)** 처리를 보장하는 것이다. 

### XA 트랜잭션

X/Open XA(eXtended Architecture)는 이종 기술에 걸친 2PC를 구현하는 표준이다. 
다음과 같은 여러 RDB와 Message Broker에서 지원된다. 

- RDB: PostgreSQL, MySQL, DB2, SQLServer, Oracle
- Message Broker: ActiveMQ, HonetQ, MSMQ, IBM MQ

XA는 네트워크 프로토콜이 아니고, 트랜잭션 코디네이터와 연결되는 인터페이스를 제공하는 API로 여러 언어에서 지원한다. 

### 의심스러운 상태에 있는 동안 잠금을 유지하는 문제

쓰기를 하는 동안 데이터베이스 트랜잭션은 더티 쓰기를 막기위해 보통 로우 수준의 잠금을 획득한다. (2단계 잠금을 활용하는 경우 읽은 로우에도 공유 잠금을 획득한다.)

트랜잭션이 의심스러운 상태에 빠지고 그 상태가 유지된다면, 해당 트랜잭션에서 사용하는 데이터에 대한 잠금이 계속 유지된다.  그리고 이러한 상태를 **트랜잭션 블록(blocked)상태**라고도 부른다.

![](https://velog.velcdn.com/images/cksgodl/post/37a81409-04cc-410a-bcd8-d077058f4ae9/image.png)

DB는 트랜잭션이 커밋되거나 어보트할 때까지 잠금을 유지하고 있는데, 만약 코디네이터가 죽어서 재시작까지 20분이 걸린다면 잠금 역시 20분간 유지된다. 심지어 코디네이터가 복구되지 못한다면 잠금은 영원히 유지될 수도 있고, 관리자가 수동으로 해결해야 한다. 

### 코디네이터 장애 복구하기

코디네이터가 죽은 후 재시작하면 의심스러운 트랜잭션을 해소해야 한다. 그러나 현실에서는 **고아가 된(orphaned)** 의심스러운 트랜잭션, 즉 코디네이터가 어떤 이유 때문인지 그 결과를 결정할 수 없는 트랜잭션이 생길 수 있다. (트랜잭션 로그 손실, 버그로 오염)

이는 관리자가 수동으로 트랜잭션을 커밋하거나 롤백할지 결정하여 해결해야 한다. 여러 XA 구현에서는 의심스러운 트랜잭션을 어보트하거나 커밋할지 일방적으로 결정할 수 있는 **경험적 결정(heuristic decision)**이라고 부르는 비상 탈출구가 있다. 
사실 여기서 말하는 경험적은 2PC의 약속 체계를 위반하기에 원자성을 깰 수도 있다. 
따라서 경험적 결정은 평소가 아닌 큰 장애 상황을 벗어나고자 할 때만 쓰도록 의도된 것이다. 

### 분산 트랜잭션의 제약

*XA 트랜잭션은 여러 참여 데이터 시스템이 서로 일관성을 유지하게 하는 실제적이고 중요한 문제를 해결해준다. *

 핵심 구현은 트랜잭션 코디네이터 자체가 트랜잭션 결과를 저장할 수 있는 일종의 `DB`여야 하고 따라서 다른 중요한 DB와 동일하게 신경써서 접근해야 한다. 
 
 
- 코디네이터가 복제되지 않고 단일 장비에서만 실행되면 전체 시스템의 단일 장애점(single point of failure)이 된다. 
* 여러 서버 사이드 애플리케이션은 모든 영속적인 상태를 DB에 저장하고 상태 비저장 모드로 개발된다. 애플리케이션 서버를 마음대로 추가 및 제거할 수 있다는 이점이 있지만,  코디네이터가 애플리케이션 서버의 일부가 되면 배포특성이 변하게 된다. 코디네이터의 로그가 지속적인 시스템 상태의 중요한 부분이 된다. 코디네이터 로그가 계속해서 관리되야 하기에 애플리케이션 서버가 더 이상 상태 비저장(stateless)이 아닌 상태 저장(stateful)하게 된다.
* 분산 버전 SSI를 쓸 수 있다. 
하지만, 트랜잭션 커밋을 위해 모든 참여자가 응답해야 한다는 문제가 남는다. 
분산 트랜잭션은 부분적인 문제도 전체로 커지기 때문에 장애를 증폭시키는 경향이 있고, 이는 내결함성을 지니는 시스템을 구축하려는 목적에 어긋난다.

> **여러 코디네이터 구현이 고가용성 미제공 및 기초적인 복제만 지원하는 이유**
>
1. 복제 비용 문제: 분산 트랜잭션에서 코디네이터 복제를 하려면 여러 개의 서버에 대한 복제를 수행해야 하는데, 이는 서버 자원 소모 및 복제된 데이터의 일관성을 유지하는데 필요한 비용도 크다.
>
2. 동기화의 어려움: 분산 환경에서 코디네이터의 동기화는 어려운 문제다. 여러 서버에서 실행되는 트랜잭션을 관리하면서 코디네이터 간에 동기화도 유지해야하는 부분은 상당히 복잡하다. 
>
3. 성능 저하: 분산 환경에서 코디네이터를 복제하면서 고가용성을 제공하려면 네트워크 대역폭이 많이 필요하고, 서버 자원도 많이 필요한데, 이 때문에 전체적인 분산 트랜잭션 처리 성능을 저하시킬 수 있다. 

### 내결함성을 지닌 합의

알고리즘에서 합의는 다음 속성을 만족한다.

* **균일한 동의**
어떤 두 노드도 다르게 결정하지 않는다.
* **무결성**
어떤 노드도 두 번 결정하지 않는다.
* **유효성**
한 노드가 값 v를 결정한다면 v는 어떤 노드에서 제안된 것이다.
* **종료**
죽지 않은 모든 노드는 결국 어떤 값을 결정한다.

### 합의 알고리즘과 전체순서 브로드캐스트

이 알고리즘 중 대다수는 실제로는 여기서 설명한 형식적 모델(동의, 무결성, 유효성, 종료 속성)을 직접 사용하지 않는다. 

*대신 값의 순차열(sequence)에 대해 결정해 전체 순서 브로드캐스트 알고리즘을 만든다.*

전체 순서 브로드캐스트는 모든 노드에게 정확히 한번, 같은 순서로 전달돼야 한다. 이 특징은 합의를 몇 회 하는 것과 동일하다. (각 합의 결정이 하나의 메시지 전달에 해당한다.)

* 합의의 동의 속성 때문에 모든 노드는 같은 메시지를 같은 순서로 전달하도록 결정한다.
* 무결성 속성 때문에 메시지는 중복되지 않는다.
* 유효성 속성 때문에 메시지는 오염되지 않고 난데없이 조작되지 않는다.
* 종료 속성 때문에 메시지는 손실되지 않는다.

뷰스탬프 복제, 라프트, 잽과 같은 합의 알고리즘은 전체 순서 브로드캐스트를 직접 구현한다. 그 이유는 한 번에 하나의 값을 처리하는 합의를 여러 번 하는 것보다 효율적이기 때문이다. 

### 단일 리더 복제와 헙의

단일 리더 복제는 모든 쓰기를 리더에게 전달하고 쓰기도 같은 순서로 팔로워에 적용하기 때문에 전체 순서 브로드캐스트와 유사하다. 하지만 리더를 어떻게 선택하냐가 다르다.

리더를 운영팀이 설정하면 이는 "독재자" 방식의 합의 알고리즘이다. 또한 이는 합의의 종료 속성을 만족하지 않는다. 

스플릿 브레인 현상을 예방하기 위해 합의가 필요하다. 하지만 전체 순서 브로드캐스트 알고리즘은 단일 리더 복제와 같기 때문에 합의를 위해선 리더가 필요하다. **즉, 리더를 선출하기 위해선 리더가 필요한 상황인 것이다.**

### 에포크 번호 붙이기와 정족수

합의는 리더를 사용하지만 리더가 유일하다고 보장하지는 않는다. 이들은 에포크 번호를 통해 더 약한 보장을 제공한다.

각 에포크 내에선 리더가 유일하다고 보장한다. 에포크 번호는 전체 순서가 있고 단조 증가한다. 그래서 두 리더 사이에 충돌이 있다면 에포크 번호가 높은 리더가 이긴다.

리더는 뭔가 결정하도록 허용하기 전 충돌되는 결정을 할지도 모르는 에포크 번호가 더 높은 다른 리더가 없는지 먼저 확인해야 한다. 리더는 자신의 결정에 대해 제안된 값을 다른 팔로워들에게 보내서 노드의 정족수가 그 제안을 찬성한다고 응답하기를 기다려야 하며, 노드는 에포크 번호가 더 높은 다른 리더를 알지 못할 때만 제안에 찬성하는 투표를 한다. 

**따라서 투표는 두 번이 있다. **

* 한 번은 리더를 선출하기 위해
* 두 번째는 리더의 제안에 투표하기 위해

**중요한건 두 번의 투표 모두 정족수가 겹쳐야 한다.** 제안에 대한 투표가 성공했다면 투표한 노드 중 최소 하나는 가장 최근의 리더 선출에도 참여했어야 한다. 따라서 제안에 대한 투표를 할 때 에포크 번호가 더 큰 것이 있다고 밝혀지지 않았다면 현재 리더는 에포크 번호가 더 높은 리더 선출이 발생하지 않았다는 결론을 내릴 수 있고, 그 결과 자신이 리더라고 확신할 수 있다.

이 투표 과정은 2PC와 비슷해보이지만, 다음과 같은 차이점이 있다. 

* 2PC의 코디네이터는 선출되지 않는다.
* 2PC는 모든 참여자로부터 네 투표가 필요하다. 

합의 알고리즘은 새로운 리더가 선출된 후 노드를 일관적인 상태로 만들어주는 복구 과정을 정의해 안전성 속성이 항상 만족되도록 보장한다.

### 협의의 제약

*합의알고리즘이 만능은 아니다.*

제안이 결정되기 전에 투표하는 과정은 데이터의 복제와 동일하다. 즉, 데이터베이스는 종종 비동기 복제를 사용한다. 따라서 투표 결과는 잠재적으로 손실될 수 있지만 사람들은 더 나은 성능을 위해 이 위험을 받아들이기로 선택했다.

합의 시스템은 항상 과반수 이상의 노드가 동작하기를 요구한다. 합의 알고리즘의 **동적 멤버십 확장**은 클러스터에 있는 노드가 시간이 지남에 따라 바뀌는 것을 허용하지만 복잡하다.

합의 시스템은 장애 노드를 감지하기위해 일반적으로 타임아웃에 의존한다. 네트워크 지연이 심한 환경에서는 잦은 리더 선출이 일어날 수 있고 이는 성능적으로 끔직하다. 즉, 합의 알고리즘은 네트워크에 민감하다.

#### 래프트의 무한루프

> 래프트 알고리즘은 특정 네트워크 링크가 불안정하다면 리더십이 두 노드사이에 왔다 갔다하면서 리더가 꾸준히 변경되는 문제점이 있다.

🔥 문제의 원인: 네트워크 불안정 + 선거 타이밍

- 래프트에서 리더는 다른 노드들에게 하트비트(heartbeat) 신호를 계속 보내면서 "내가 리더다!"라고 알린다

근데 특정 네트워크 링크가 불안정하다면?? 😱

1️⃣ 리더가 일부 팔로워와의 연결이 자꾸 끊김 → 팔로워는 리더가 죽었다고 착각하고 새로운 리더 선거를 시작한다
2️⃣ 새 리더가 당선됨 → 하지만 기존 리더가 여전히 살아 있어서 다시 하트비트를 보내기 시작함
3️⃣ 일부 노드가 다시 기존 리더를 따르지만, 또다시 네트워크가 끊기면 새로운 선거 발생
4️⃣ 이 과정이 반복되면서 리더가 계속 바뀌는 상태(즉, 불안정한 리더십)가 발생!

이게 바로 리더십이 두 노드 사이에서 왔다 갔다 하는 이유이다.

### 멤버십과 코디네이션 서비스

HBase, 하둡 얀, 카프카 등은 배후에서 주키퍼에 의존한다.  

주키퍼와 `etcd`는 모두 메모리 안에 들어올 수 있는 작은 양의 데이터를 보관하도록 설계된다.(지속성을 위해 디스크를 사용하긴 한다.)
그러나 몇몇 데이터는 전체 순서 브로드캐스트 알고리즘을 사용해 모든 노드에 걸쳐 복제된다. 

주키퍼에서 제공하는 기능은 다음과 같다.

- 선형성 원자적 연산
  - compare-and-set 연산을 통해 잠금을 구현한다. (클라이언트 장애가 날 경우 타임아웃 이 있는 임차권으로 구현된다.)
- 연산의 전체 순서화
  - 클라이언트 끼리 충돌하는 것을 막기 위한 펜싱토큰을 사용한다. 
  - 주키퍼는 모든 연산에 전체 순서를 정하고 각 연산에 단조 증가하는 트랜잭션 ID(zxid)와 버전 번호(cversion)을 할당해 제공한다.
- 장애 감지
  - 하트비트를 통해 수명을 확인한다. (이를 단명 노드라고 한다.)
- 변경 알림
  - 주키퍼는 노드에 변경이 있는지 감시한다. 
  - 주키퍼에서 이를 구독함으로써 클라이언트는 변경을 발견하기 위해 주기적으로 폴링하지 않아도 된다.
  
### 작업을 노드에 할당하기

주키퍼가 하는 작업은 다음과 같다.

- 리더를 넘겨주기
- 데이터를 파티셔닝 하기
  - 리파티셔닝 하기
- 장애 났을 때 단명 노드 파악하기

등 이런 작업은 원자적 연산, 단명 노드, 알림을 신중하게 사용하면 해낼 수 있다.

어플리케이션은 단일 노드로 실행될지 모르지만, 나중에 수천개의 노드로 늘어날 수 있다. 이런 노드에서 과반수 투표를 수행하는 것은 매우 비효율적이다. 따라서 주키퍼는 보통 3개, 5개의 고정된 노드에서 실행되고 이 노드들 사이에서 과반수 투표를 수행하여 많아질 수 있는 클라이언트를 지원한다.

주키퍼로 관리하는 데이터는 느리게 변화한다. (리더의 상태가 쉽게 변하지 않는다.) 


### 서비스 찾기

주키퍼는 **서비스 찾기(service discovery)**, 즉 특정 서비스에 연결하려면 어떤 IP주소로 접속해야 하는지 알아내는 용도로도 자주 사용된다.  (서비스 라우팅 역할)

서비스 찾기에 합의가 필요한지 분명해 보이지 않는다. 예를 들어 DNS는 서비스 이름으로 IP 주소를 찾는 전통적인 방법이고, 다중 캐시를 사용한다. 

서비스 찾기는 합의는 필요 없지만 리더 선출에 합의가 필요하다. 합의 시스템이 누가 리더인지 안다면 다른 서비스들이 리더가 누구인지 찾는 데 그 정보를 사용하는 것도 타당하다. 따라서 어떤 합의 시스템은 읽기 전용 캐시 복제 서버를 지원한다. 이 복제 서버는 합의 알고리즘의 모든 결정에 관한 로그를 비동기로 받지만 투표에 참여하지 않는다. 

---

#### 🦁 주키퍼의 서비스 찾기란?

분산 시스템에서는 여러 개의 서비스(서버)가 존재하고, 클라이언트는 어떤 서버가 현재 살아 있는지, 어디로 요청을 보내야 하는지 알아야 한다
> 💡 ZooKeeper는 이런 정보를 관리하고, 클라이언트가 동적으로 서비스를 찾을 수 있도록 도와주는 역할을 한다.

- 🏗 어떻게 동작하는가?

1️⃣ 서비스 등록
각 서비스(예: 마이크로서비스, 데이터베이스 등)는 ZooKeeper에 자신의 정보를 등록한다
서비스는 특정 **ZNode(주키퍼의 노드 개념)**에 자신의 IP, 포트 등을 저장함
예를 들어, /services/api-service 노드 아래에 여러 개의 인스턴스가 등록될 수 있음
```bash
/services/api-service/instance1 -> "192.168.1.10:8080"
/services/api-service/instance2 -> "192.168.1.11:8080"
```
2️⃣ 클라이언트가 서비스 찾기
클라이언트는 ZooKeeper에서 해당 서비스의 노드 목록을 가져옴
그러면 여러 개의 인스턴스 중 하나를 선택해서 요청을 보낼 수 있음 (ex: 로드 밸런싱 가능)

3️⃣ 서비스 상태 모니터링 (자동 감지)
ZooKeeper의 강력한 기능 **Ephemeral Node(일시적인 노드)**를 이용하면?
서비스가 죽으면 자동으로 노드 삭제됨
클라이언트가 워처(Watcher)를 등록해 두면, 노드 변경 사항을 바로 감지할 수 있음

### 멤버십 서비스란?

> 💡 "현재 살아 있는 노드(서버) 목록을 관리하는 서비스"
분산 시스템에서는 노드들이 동적으로 추가되거나 제거될 수 있음

- 새 노드가 참여하면? 👉 클러스터 멤버에 추가해야 함
- 기존 노드가 죽으면? 👉 클러스터에서 제거해야 함

이런 노드의 가입/탈퇴를 감지하고 관리하는 역할을 하는 게 바로 멤버십 서비스다

주키퍼이외에도 다양한 곳에서 사용된다.
✔ 분산 데이터베이스 (ex: Apache Cassandra, etcd, ZooKeeper)
✔ 컨테이너 오케스트레이션 (ex: Kubernetes)
✔ 마이크로서비스 아키텍처
✔ 리더 선출 알고리즘 (ex: Raft, Paxos)

> 🔥 멤버십 서비스가 중요한 이유
✔ 장애 감지 → 장애 발생 시 빠르게 감지하고 처리 가능
✔ 자동 확장 지원 → 노드가 동적으로 추가/제거되어도 안정적으로 운영 가능
✔ 리더 선출 가능 → 클러스터 멤버 정보를 기반으로 리더 선출을 할 수 있음
✔ 부하 분산 → 살아 있는 노드만 대상으로 로드 밸런싱 가능

## 정리

타임스탬프를 통한 순서화로는 충분하지 않다. 이러한 선형화는 우리를 합의로 이끈다. 합의가 필요한 예제는 다음과 같다.

- 선형적 compare-and-set
  - 레지스터는 현재 값이 매개변수로 넘겨진 값과 같은지 원자적으로 결정해야 한다.
- 원자적 트랜잭션 커밋
   - 데이터베이스는 분산 트랜잭션을 어보트할 것인지 커밋할지 결정해야 한다.
- 전체 순서 브로드캐스트
   - 메시징 시스템은 메시지를 전달할 순서를 결정해야 한다.
- 잠금과 임차권
  - 클라이언트 중 누가 잠금을 획득할지 결정해야 한다.
- 멤버십/코디네이션 서비스
  - 장애가 일어나면 어떤 노드가 죽었는지 결정해야 한다.
- 유일성 제약 조건
  - 트랜잭션들이 동시에 같은 키로 충돌할 때 어떤 것을 실패시킬지 결정해야 한다.
  
  
단일 노드를 쓰면 이러한 문제는 쉽게 해결된다. (하지만 이런 상황은 흔치 않다.)

> 주키퍼 같은 도구는 어플리케이션이 사용할 수 있는 합의, 장애 감지, 멤버십 서비스를 "위탁"하는데 중요한 역할을 수행한다. 내결함성을 지니기 원한다면 주키퍼를 쓰는 것이 현명하다.

리더 없는 시스템일 경우 합의가 필요하지 않는다. 해당 충돌이 괜찮은 서비스에서 해당 구조를 사용해야 한다.


